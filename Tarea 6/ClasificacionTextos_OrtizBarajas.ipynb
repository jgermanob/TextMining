{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación de textos\n",
    "\n",
    "En esta clase, exploraremos datos de mensajes de texto y crearemos modelos para predecir si un mensaje es spam o no."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliotecas necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  Go until jurong point, crazy.. Available only ...       0\n",
       "1                      Ok lar... Joking wif u oni...       0\n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...       1\n",
       "3  U dun say so early hor... U c already then say...       0\n",
       "4  Nah I don't think he goes to usf, he lives aro...       0\n",
       "5  FreeMsg Hey there darling it's been 3 week's n...       1\n",
       "6  Even my brother is not like to speak with me. ...       0\n",
       "7  As per your request 'Melle Melle (Oru Minnamin...       0\n",
       "8  WINNER!! As a valued network customer you have...       1\n",
       "9  Had your mobile 11 months or more? U R entitle...       1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_data = pd.read_csv('spam.csv')\n",
    "spam_data['target'] = np.where(spam_data['target']=='spam',1,0)\n",
    "spam_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### División de datos en conjuntos de entrenamiento y prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(spam_data['text'], spam_data['target'], random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 1\n",
    "¿Qué porcentaje de los documentos en `spam_data` son spam?\n",
    "\n",
    "*Esta función debe devolver un valor flotante, el valor porcentual (es decir, $ ratio * 100 $).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def respuesta_uno():\n",
    "    spam_data['target'].mean()\n",
    "    spam=spam_data[spam_data['target'] != 0]\n",
    "    print(len(spam), len(spam_data))\n",
    "    return spam_data['target'].mean()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "747 5572\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13.406317300789663"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta_uno()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 2\n",
    "\n",
    "Ajustar y transformar los datos de entrenamiento `X_train` utilizando un `count_vectorizer` con parámetros predeterminados.\n",
    "\n",
    "Luego, ajuste un modelo de clasificación Naive Bayes multinomial. Calcule medidas de exactitud, presición, recall y f1-score usando los datos de prueba transformados.\n",
    "\n",
    "*Esta función debe devolver las cuatro medidas de evaluación como una lista con los 4 valores en el orden solicitado cada valor como flotante.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def respuesta_dos():\n",
    "    scores=[]\n",
    "    vect = CountVectorizer().fit(X_train)\n",
    "    X_train_vectorized=vect.transform(X_train)\n",
    "    \n",
    "    clf=MultinomialNB()\n",
    "    clf.fit(X_train_vectorized, y_train)\n",
    "    print(\"matriz Frecuencias:\",X_train_vectorized.shape, \"clases\",len(y_train))\n",
    "    \n",
    "    X_test_vectorized=vect.transform(X_test)\n",
    "    predictions = clf.predict(X_test_vectorized)\n",
    "    print(\"matriz Frecuencias Test:\",X_test_vectorized.shape, \"prediction\",len(predictions))\n",
    "    \n",
    "    scores.append(accuracy_score(y_test,predictions))\n",
    "    scores.append(precision_score(y_test,predictions))\n",
    "    scores.append(recall_score(y_test,predictions))\n",
    "    scores.append(f1_score(y_test,predictions))\n",
    "    \n",
    "    spam=predictions[predictions!= 0]\n",
    "    print('Predicciones Spam:',len(spam))\n",
    "    print('Porcentaje Spam:',len(spam)/len(predictions)*100)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()\n",
    "    print(confusion_matrix(y_test, predictions))\n",
    "    print(tn, fp, fn, tp)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matriz Frecuencias: (4179, 7354) clases 4179\n",
      "matriz Frecuencias Test: (1393, 7354) prediction 1393\n",
      "Predicciones Spam: 184\n",
      "Porcentaje Spam: 13.208901651112706\n",
      "[[1193    3]\n",
      " [  16  181]]\n",
      "1193 3 16 181\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9863603732950467,\n",
       " 0.9836956521739131,\n",
       " 0.9187817258883249,\n",
       " 0.9501312335958005]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta_dos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 3\n",
    "\n",
    "Ajustar y transformar los datos de entrenamiento `X_train` utilizando un `count_vectorizer` con parámetros predeterminados.\n",
    "\n",
    "Luego, ajuste un modelo de clasificación Naive Bayes multinomial con suavizado (smoothing) `alpha = 0.1`. Encuentre el área bajo la curva (AUC) usando los datos de prueba transformados.\n",
    "\n",
    "*Esta función debe devolver el AUC como un flotante.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def respuesta_tres():\n",
    "    vect = CountVectorizer().fit(X_train)\n",
    "    X_train_vectorized=vect.transform(X_train)\n",
    "    clf=MultinomialNB(alpha=0.1)\n",
    "    clf.fit(X_train_vectorized, y_train)\n",
    "    X_test_vectorized=vect.transform(X_test)\n",
    "    predictions = clf.predict(X_test_vectorized)\n",
    "    print(predictions)\n",
    "    return roc_auc_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9720812182741116"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta_tres()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 4\n",
    "\n",
    "Ajustar y transformar los datos de entrenamiento `X_train` utilizando un `TfidfVectorizer` ignorando los términos que tienen una frecuencia de documento estrictamente inferior a **3**.\n",
    "\n",
    "Luego, ajuste un modelo de clasificador Naive Bayes multinomial con suavizado (smoothing) `alfa = 0.1` y calcule el área bajo de la curva (AUC) usando los datos de prueba transformados.\n",
    "\n",
    "*Esta función debe devolver el AUC como un flotante.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def respuesta_cuatro():\n",
    "    vect = TfidfVectorizer(min_df=3).fit(X_train)\n",
    "    X_train_vectorized=vect.transform(X_train)\n",
    "    clf=MultinomialNB(alpha=0.1)\n",
    "    clf.fit(X_train_vectorized, y_train)\n",
    "    X_test_vectorized=vect.transform(X_test)\n",
    "    predictions = clf.predict(X_test_vectorized)\n",
    "    return roc_auc_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9416243654822335"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta_cuatro()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 5\n",
    "\n",
    "¿Cuál es la longitud promedio de los documentos (número de caracteres) para los documentos spam y no spam?\n",
    "\n",
    "*Esta función debe devolver una tupla (longitud promedio no spam, longitud promedio de spam).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_counter(text_list):\n",
    "    counter = []\n",
    "    for text in text_list:\n",
    "        counter.append(len(text))\n",
    "    return counter        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def respuesta_cinco(df):\n",
    "    # Filtrado el dataframe #\n",
    "    df_spam = df[(df.target == 1)] \n",
    "    df_no_spam = df[(df.target == 0)]\n",
    "    spam_counter = char_counter(df_spam['text'])\n",
    "    no_spam_counter = char_counter(df_no_spam['text'])\n",
    "    return (sum(no_spam_counter)/len(df_no_spam.index), sum(spam_counter)/len(df_spam.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71.02362694300518, 138.8661311914324)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta_cinco(spam_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "The following function has been provided to help you combine new features into the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature(X, feature_to_add):\n",
    "    \"\"\"\n",
    "    Returns sparse feature matrix with added feature.\n",
    "    feature_to_add can also be a list of features.\n",
    "    \"\"\"\n",
    "    \n",
    "    return hstack([X, csr_matrix(feature_to_add).T], 'csr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 6\n",
    "\n",
    "Ajustar y transformar los datos de entrenamiento `X_train` usando un `TfidfVectorizer` ignorando los términos que tienen una frecuencia de documento estrictamente inferior a **5**.\n",
    "\n",
    "Usando esta matriz de término de documento y una característica adicional, **la longitud del documento (número de caracteres)**, ajustar a un modelo de Clasificación de Vector de Soporte con regularización `C = 10000`. Luego calcule el área bajo de la curva (AUC) usando los datos de prueba transformados.\n",
    "\n",
    "*Esta función debe devolver el AUC como un flotante.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def respuesta_seis():\n",
    "    # Obtención de longitud de los documentos de entrenamiento y prueba #\n",
    "    document_len_train = np.array(char_counter(X_train))\n",
    "    document_len_test = np.array(char_counter(X_test))\n",
    "    \n",
    "    # Obtención de valores TF-IDF para entrenamiento y prueba #\n",
    "    vect = TfidfVectorizer(min_df=5).fit(X_train)\n",
    "    X_train_vectorized=vect.transform(X_train)\n",
    "    X_test_vectorized=vect.transform(X_test)\n",
    "    \n",
    "    print(X_train_vectorized.shape)\n",
    "    \n",
    "    #Concatenación de características#\n",
    "    X_train_concat = add_feature(X_train_vectorized, document_len_train)\n",
    "    X_test_concat = add_feature(X_test_vectorized, document_len_test)\n",
    "    \n",
    "    print(X_train_concat.shape)\n",
    "    \n",
    "    #Entrenamiento y prueba del clasificador#\n",
    "    clf = SVC(C = 10000)\n",
    "    clf.fit(X_train_concat, y_train)\n",
    "    predictions = clf.predict(X_test_concat)\n",
    "    \n",
    "    return roc_auc_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4179, 1468)\n",
      "(4179, 1469)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9661689557407943"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta_seis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 7\n",
    "\n",
    "¿Cuál es el número promedio de dígitos por documento para los documentos no spam y spam?\n",
    "\n",
    "*Esta función debe devolver una tupla (promedio de # dígitos no es spam, promedio # dígitos spam).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_counter(text_list, tokenizer):\n",
    "    counter = []\n",
    "    for text in text_list:\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        counter.append(len(tokens))\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def respuesta_siete(df):\n",
    "    # Filtrado el dataframe #\n",
    "    df_spam = df[(df.target == 1)] \n",
    "    df_no_spam = df[(df.target == 0)]\n",
    "    # conteo de digitos #\n",
    "    digit_tokenizer = RegexpTokenizer(r'\\d')\n",
    "    spam_counter = get_feature_counter(df_spam['text'], digit_tokenizer)\n",
    "    no_spam_counter = get_feature_counter(df_no_spam['text'], digit_tokenizer)\n",
    "    return (sum(no_spam_counter)/len(df_no_spam.index), sum(spam_counter)/len(df_spam.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2992746113989637, 15.759036144578314)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta_siete(spam_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 8\n",
    "\n",
    "Ajustar y transformar los datos de entrenamiento `X_train` usando un `TfidfVectorizer` ignorando los términos que tienen una frecuencia de documento estrictamente inferior a **5** y usando **n-gramas de palabras n = 1 a n = 3** (unigramas, bigramas y trigramas).\n",
    "\n",
    "Usando esta matriz de término-documento y las siguientes características adicionales:\n",
    "* la longitud del documento (número de caracteres)\n",
    "* **cantidad de dígitos por documento**\n",
    "\n",
    "Ajustar un modelo de Regresión logística con regularización `C = 100`. Luego calcule el área bajo de la curva (AUC) usando los datos de prueba transformados.\n",
    "\n",
    "*Esta función debe devolver el AUC como un flotante.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def respuesta_ocho():\n",
    "    # Obtención de longitud de los documentos de entrenamiento y prueba #\n",
    "    document_len_train = np.array(char_counter(X_train))\n",
    "    document_len_test = np.array(char_counter(X_test))\n",
    "    \n",
    "    # Obtención de digitos por documento de entrenamiento y prueba#\n",
    "    digit_tokenizer = RegexpTokenizer(r'\\d')\n",
    "    document_digits_train = np.array(get_feature_counter(X_train, digit_tokenizer))\n",
    "    document_digits_test = np.array(get_feature_counter(X_test, digit_tokenizer))\n",
    "    \n",
    "    # Obtención de valores TF-IDF para entrenamiento y prueba #\n",
    "    vect = TfidfVectorizer(min_df=5, ngram_range=(1,3)).fit(X_train)\n",
    "    X_train_vectorized=vect.transform(X_train)\n",
    "    X_test_vectorized=vect.transform(X_test)\n",
    "    \n",
    "    print(X_train_vectorized.shape)\n",
    "    \n",
    "    #Concatenación de características#\n",
    "    X_train_concat_char = add_feature(X_train_vectorized, document_len_train)\n",
    "    X_test_concat_char = add_feature(X_test_vectorized, document_len_test)\n",
    "    \n",
    "    X_train_concat = add_feature(X_train_concat_char, document_digits_train)\n",
    "    X_test_concat = add_feature(X_test_concat_char, document_digits_test)\n",
    "    \n",
    "    print(X_train_concat.shape)\n",
    "    \n",
    "    #Entrenamiento y prueba del clasificador#\n",
    "    clf = LogisticRegression(C = 100)\n",
    "    clf.fit(X_train_concat, y_train)\n",
    "    predictions = clf.predict(X_test_concat)\n",
    "    \n",
    "    return roc_auc_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4179, 3383)\n",
      "(4179, 3385)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/german/.local/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9759031798040846"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta_ocho()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 9\n",
    "\n",
    "¿Cuál es el número promedio de caracteres que no son palabras (cualquier cosa que no sea una letra, un dígito o un guión bajo) por documento para los documentos que no son spam y spam?\n",
    "\n",
    "*Sugerencia: utilice las clases de caracteres `\\ w` y` \\ W`*\n",
    "\n",
    "*Esta función debe devolver una tupla (promedio de # caracteres que no son palabras, no spam, promedio de # caracteres que no son palabras, spam).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pregunta_nueve(df):\n",
    "    # Filtrado el dataframe #\n",
    "    df_spam = df[(df.target == 1)] \n",
    "    df_no_spam = df[(df.target == 0)]\n",
    "    # conteo de digitos #\n",
    "    specialchar_tokenizer = RegexpTokenizer(r'\\w')\n",
    "    spam_counter = get_feature_counter(df_spam['text'], specialchar_tokenizer)\n",
    "    no_spam_counter = get_feature_counter(df_no_spam['text'], specialchar_tokenizer)\n",
    "    return (sum(no_spam_counter)/len(df_no_spam.index), sum(spam_counter)/len(df_spam.index)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53.73181347150259, 109.82463186077644)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pregunta_nueve(spam_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 10\n",
    "\n",
    "Ajustar y transformar los datos de entrenamiento `X_train` usando un `CountVectorizer` ignorando los términos que tienen una frecuencia de documento estrictamente inferior a **5** y utilizando **caracteres n-grams desde n = 2 a n = 5.**\n",
    "\n",
    "Para decirle a `CountVectorizer` que use caracteres n-grams, pase en `analyzer = 'char_wb'` que crea caracteres n-gramas solo del texto dentro de los límites de las palabras. Esto debería hacer que el modelo sea más robusto a los errores ortográficos.\n",
    "\n",
    "Usando esta matriz término documento y las siguientes características adicionales:\n",
    "* la longitud del documento (número de caracteres)\n",
    "* cantidad de dígitos por documento\n",
    "* **cantidad de caracteres que no son palabras (cualquier cosa que no sea una letra, dígito o guión bajo).**\n",
    "\n",
    "Ajustar un modelo de Regresión logística con regularización `C = 100`. Luego calcule el área bajo de la curva (AUC) usando los datos de prueba transformados.\n",
    "\n",
    "También **encuentre los 10 coeficientes más pequeños y los 10 más grandes del modelo** y devuélvalos junto con el AUC en una tupla.\n",
    "\n",
    "La lista de los 10 coeficientes más pequeños debe ordenarse primero, la lista de los 10 coeficientes más grandes debe ordenarse primero.\n",
    "\n",
    "Las tres características que se agregaron a la matriz de términos del documento deben tener los siguientes nombres si aparecen en la lista de coeficientes:\n",
    "['longitud_doc', 'conteo_digito', 'caracteres_no_palabra']\n",
    "\n",
    "*Esta función debe devolver una tupla `(AUC como flotante, lista de coeficientes más pequeños, lista de coeficientes más grande)`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def respuesta_10():\n",
    "    # Obtención de longitud de los documentos de entrenamiento y prueba #\n",
    "    document_len_train = np.array(char_counter(X_train))\n",
    "    document_len_test = np.array(char_counter(X_test))\n",
    "    \n",
    "    # Obtención de digitos por documento de entrenamiento y prueba#\n",
    "    digit_tokenizer = RegexpTokenizer(r'\\d')\n",
    "    document_digits_train = np.array(get_feature_counter(X_train, digit_tokenizer))\n",
    "    document_digits_test = np.array(get_feature_counter(X_test, digit_tokenizer))\n",
    "    \n",
    "    # Obtención de caracteres especiales por documento de entrenamiento y prueba#\n",
    "    specialchar_tokenizer = RegexpTokenizer(r'\\w')\n",
    "    document_specialchar_train = np.array(get_feature_counter(X_train, specialchar_tokenizer))\n",
    "    document_specialchar_test = np.array(get_feature_counter(X_test, specialchar_tokenizer))\n",
    "    \n",
    "    # Obtención de valores Countvectorizer para entrenamiento y prueba #\n",
    "    vect = TfidfVectorizer(min_df=5, ngram_range=(2,5), analyzer='char_wb').fit(X_train)\n",
    "    X_train_vectorized=vect.transform(X_train)\n",
    "    X_test_vectorized=vect.transform(X_test)\n",
    "    \n",
    "    print(X_train_vectorized.shape)\n",
    "    \n",
    "    #Concatenación de características#\n",
    "    X_train_concat_char = add_feature(X_train_vectorized, document_len_train)\n",
    "    X_test_concat_char = add_feature(X_test_vectorized, document_len_test)\n",
    "    \n",
    "    X_train_concat_digit = add_feature(X_train_concat_char, document_digits_train)\n",
    "    X_test_concat_digit = add_feature(X_test_concat_char, document_digits_test)\n",
    "    \n",
    "    X_train_concat = add_feature(X_train_concat_digit, document_specialchar_train)\n",
    "    X_test_concat = add_feature(X_test_concat_digit, document_specialchar_test)\n",
    "    \n",
    "    print(X_train_concat.shape)\n",
    "    \n",
    "    #Entrenamiento y prueba del clasificador#\n",
    "    clf = LogisticRegression(C = 100)\n",
    "    clf.fit(X_train_concat, y_train)\n",
    "    predictions = clf.predict(X_test_concat)\n",
    "    \n",
    "    added_features = ['longitud_doc', 'conteo_digito', 'caracteres_no_palabra']\n",
    "    features_coefs = clf.coef_[0][-3:]\n",
    "    \n",
    "    features_dict = dict(zip(added_features, features_coefs))\n",
    "    \n",
    "    sorted_coef = sorted(clf.coef_[0])\n",
    "    \n",
    "    min_coef = sorted_coef[:10]\n",
    "    max_coef = sorted_coef[-10:]\n",
    "    \n",
    "    for feature in added_features:\n",
    "        for index in range(10):\n",
    "            if min_coef[index] == features_dict.get(feature):\n",
    "                min_coef[index] = feature\n",
    "            elif max_coef[index] == features_dict.get(feature):\n",
    "                max_coef[index] = feature\n",
    "                \n",
    "    print(features_dict)\n",
    "    \n",
    "    return (roc_auc_score(y_test,predictions), min_coef, max_coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4179, 16314)\n",
      "(4179, 16317)\n",
      "{'longitud_doc': -0.1399951687319573, 'conteo_digito': 0.7458820324653969, 'caracteres_no_palabra': 0.1917936152108704}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/german/.local/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9805612617353955,\n",
       " [-3.945912526988008,\n",
       "  -3.247943281242045,\n",
       "  -3.22616162914201,\n",
       "  -3.046142138087868,\n",
       "  -2.897679896859826,\n",
       "  -2.699966521619068,\n",
       "  -2.6325556556724936,\n",
       "  -2.5363792443320543,\n",
       "  -2.4709558067322144,\n",
       "  -2.3698342812270363],\n",
       " [3.695829977143551,\n",
       "  3.8744466806017037,\n",
       "  3.9138516154990723,\n",
       "  3.943569390418778,\n",
       "  3.9492591298235653,\n",
       "  4.4219593305637055,\n",
       "  4.46309756059203,\n",
       "  4.512965699609701,\n",
       "  4.732352683166704,\n",
       "  5.408390926803798])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta_10()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "python-text-mining",
   "graded_item_id": "Pn19K",
   "launcher_item_id": "y1juS",
   "part_id": "ctlgo"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
